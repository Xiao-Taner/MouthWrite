import pyaudio
import wave
import base64
import threading
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================
# WSL2 çš„åœ°å€ (vLLM æœåŠ¡)
API_BASE = "http://localhost:8000/v1"
API_KEY = "EMPTY"
MODEL_NAME = "Qwen/Qwen3-ASR-1.7B"

# å½•éŸ³é…ç½®
CHUNK = 1024
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000  # Qwen3-ASR æ¨è 16k é‡‡æ ·ç‡
WAVE_OUTPUT_FILENAME = "temp_record.wav"
# ===========================================

def record_audio():
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)

    print("\nğŸ¤ æ­£åœ¨å½•éŸ³... (æŒ‰ä¸‹å›è½¦é”®åœæ­¢)")
    frames = []
    
    # ä½¿ç”¨ä¸€ä¸ªæ ‡å¿—ä½æ¥æ§åˆ¶å½•éŸ³çº¿ç¨‹
    is_recording = True

    def input_thread():
        nonlocal is_recording
        input() # ç­‰å¾…ç”¨æˆ·æŒ‰å›è½¦
        is_recording = False

    # å¯åŠ¨ç›‘å¬é”®ç›˜çš„çº¿ç¨‹
    threading.Thread(target=input_thread).start()

    while is_recording:
        data = stream.read(CHUNK)
        frames.append(data)

    print("ğŸ›‘ å½•éŸ³ç»“æŸï¼Œæ­£åœ¨å‘é€ç»™ AI...")

    stream.stop_stream()
    stream.close()
    p.terminate()

    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))
    wf.close()

def audio_to_base64(file_path):
    with open(file_path, "rb") as audio_file:
        return base64.b64encode(audio_file.read()).decode('utf-8')

def call_qwen_asr():
    client = OpenAI(base_url=API_BASE, api_key=API_KEY)
    
    # æŠŠéŸ³é¢‘æ–‡ä»¶è½¬ä¸º Base64 å­—ç¬¦ä¸²ï¼Œè¿™æ ·æ— éœ€ä¸Šä¼ æ–‡ä»¶å®ä½“
    base64_audio = audio_to_base64(WAVE_OUTPUT_FILENAME)
    data_url = f"data:audio/wav;base64,{base64_audio}"

    try:
        # å‘èµ·æµå¼è¯·æ±‚ (stream=True)
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": "è¯·å°†è¿™æ®µè¯­éŸ³è½¬å½•ä¸ºæ–‡å­—ï¼š"}, # å¯é€‰çš„ Prompt
                        {"type": "image_url", "image_url": {"url": data_url}} # vLLMè¿™é‡Œå€Ÿç”¨äº†image_urlçš„å­—æ®µä¼ éŸ³é¢‘
                    ]
                }
            ],
            stream=True, # <--- å…³é”®ï¼šå¼€å¯æµå¼è¾“å‡º
            temperature=0.0 # ASR ä¸éœ€è¦åˆ›é€ æ€§ï¼Œæ¸©åº¦è®¾ä¸º0æœ€å‡†
        )

        print("\nğŸ“ è¯†åˆ«ç»“æœï¼š")
        print("-" * 30)
        
        full_text = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True) # æ‰“å­—æœºæ•ˆæœ
                full_text += content
        
        print("\n" + "-" * 30 + "\n")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("æç¤ºï¼šè¯·æ£€æŸ¥ WSL2 ä¸­çš„ vLLM æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œï¼Œä¸”ç«¯å£æ˜¯ 8000ã€‚")

if __name__ == "__main__":
    while True:
        choice = input("æŒ‰å›è½¦å¼€å§‹å½•éŸ³ (è¾“å…¥ 'q' é€€å‡º): ")
        if choice.lower() == 'q':
            break
        
        record_audio()
        call_qwen_asr()